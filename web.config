<?xml version="1.0" encoding="utf-8" ?>
<configuration>
  <system.webServer>
    <staticContent>
      <remove fileExtension=".map" />
      <mimeMap fileExtension=".map" mimeType="application/json" />
    </staticContent>
    <rewrite>
      <rules>
        <rule name="Redirect to non-www" stopProcessing="true">
            <match url="(.*)" negate="false"></match>
            <action type="Redirect" url="http://notesbyv.com/{R:1}"></action>
            <conditions>
                <add input="{HTTP_HOST}" pattern="^notesbyv\.com$" negate="true"></add>
            </conditions>
        </rule>
        <rule name="Rewrite Text Requests for react router" stopProcessing="true">
          <match url=".*" />
          <conditions>
            <add input="{HTTP_METHOD}" pattern="^GET$" />
            <!-- Spotted on SO:

            This works in most cases, but I had an issue with facebook crawler where it would receive a 404. It turns out that the Accept header is set to */* when the crawler hits a site with the {HTTP_ACCEPT} rule above. This causes a 404 not found to display on facebook when sharing a link for example. Removing the {HTTP_ACCEPT} rule fixed it for me as verified here. Any gotchas with removing that condition? â€“

            http://stackoverflow.com/questions/35533291/how-to-use-react-router-createbrowserhistory-on-microsoft-iis-to-make-routing
            -->
            <add input="{HTTP_ACCEPT}" pattern="^text/html" />
            <add input="{REQUEST_FILENAME}" matchType="IsFile" negate="true" />
          </conditions>
          <action type="Rewrite" url="/index.html" />
        </rule>
      </rules>
    </rewrite>
  </system.webServer>
</configuration>